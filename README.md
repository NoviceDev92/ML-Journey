# ML-Journey

This repository serves as a portfolio of my ongoing machine learning learning journey. It contains hands-on implementations of various fundamental algorithms, developed to solidify my understanding of core concepts and build practical skills.

# My Machine Learning Learning Journey

This repository documents my projects and progress in machine learning. It contains hands-on implementations of various algorithms to deepen my understanding of the concepts.

## Projects

### 1. Linear Regression

* **Description:** An implementation of linear regression to predict a continuous target variable.
* **Methods Used:**
    * **Gradient Descent:** An iterative optimization algorithm used to find the minimum of a cost function.
    * **Normal Equations:** A closed-form solution to find the parameters that minimize the cost function.

### 2. Logistic Regression

* **Description:** An implementation of logistic regression for a classification task.
* **Methods Used:**
    * **Gradient Ascent:** An iterative optimization algorithm used to find the maximum of a likelihood function.
    * **Newton-Raphson Method:** An iterative method for finding the roots of a function, adapted here for optimization.

### 3. Softmax Regression

* **Description:** An implementation of softmax regression for a multi-class classification task on the Digits dataset.
* **Methods Used:**
    * **Softmax Function:** A generalization of the sigmoid function that converts raw scores into a probability distribution over multiple classes.
    * **Cross-Entropy Loss:** A standard loss function used to measure the performance of a classification model.
    * **Gradient Descent:** An iterative optimization algorithm used to minimize the loss function and learn the model's parameters.

### 4. GDA (Gaussian Discriminant Analysis)

* **Description:** An implementation of a generative GDA model for a classification task, with a focus on statistical assumptions.
* **Methods Used:**
    * **Bayes' Theorem:** The fundamental principle used to calculate the posterior probability for each class.
    * **Multivariate Gaussian Distribution:** A statistical assumption that the data for each class follows a specific distribution, used to calculate likelihood.
    * **Log-Likelihood:** A numerically stable method for calculating probabilities that avoids floating-point underflow.

Feel free to explore the notebooks and provide feedback!
